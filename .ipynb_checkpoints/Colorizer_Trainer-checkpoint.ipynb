{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import TensorflowUtils as utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space for global training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width and height of input images \n",
    "input_size = 128\n",
    "\n",
    "# batch size for training\n",
    "batch_size = 40  \n",
    "\n",
    "# number of epochs to train for \n",
    "epoch_num = 140  \n",
    "\n",
    "# learning rate for training\n",
    "lr = 1e-4   \n",
    "\n",
    "# for the AdamOptimizer\n",
    "beta = .9\n",
    "\n",
    "# directory for input images\n",
    "input_directory = 'Data/SubsetLandscapeData/'\n",
    "\n",
    "# directory for checkpoints (save / restore models)\n",
    "# download the l2 loss error model if necessary \n",
    "l2_checkpoints      = \"Models/CheckpointsL2/\"\n",
    "checkpoints_l2_link = \"https://www.dropbox.com/s/0nv5qm7f3h06avv/CheckpointsL2.zip?dl=1\"\n",
    "utils.maybe_download_and_extract(\"Models\", checkpoints_l2_link, is_zipfile=True)\n",
    "\n",
    "# download the huber loss error model if necessary \n",
    "huber_checkpoints      = \"Models/CheckpointsHuber/\"\n",
    "checkpoints_huber_link = \"https://www.dropbox.com/s/r1k1dol4wzwdaqe/CheckpointsHuber.zip?dl=1\"\n",
    "utils.maybe_download_and_extract(\"Models\", checkpoints_huber_link, is_zipfile=True)\n",
    "\n",
    "# download the pairwise mean squared error model if necessary \n",
    "pmse_checkpoints     = \"Models/CheckpointsPairwiseMSE\"\n",
    "checkpoints_mse_link = \"https://www.dropbox.com/s/xux39xsmgyo0y1r/CheckpointsPairwiseMSE.zip?dl=1\"\n",
    "utils.maybe_download_and_extract(\"Models\", checkpoints_mse_link, is_zipfile=True)\n",
    "\n",
    "# set checkpoints variable to the proper checkpoint\n",
    "checkpoints = pmse_checkpoints\n",
    "\n",
    "# where to find and store the pre-trained VGG model \n",
    "model_dir = \"Models/VGGModel/\"\n",
    "model_url = 'http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes the LAB layers and outputs the RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes the L, A, B, channels --> concatenates them, and converts them to RGB\n",
    "def labChannelsToRGB(l, a, b): \n",
    "    l[l > 99] = 99\n",
    "    new_lab = np.stack((l, a, b), axis=2)\n",
    "    new_lab = new_lab.astype('float64');\n",
    "    return color.lab2rgb(new_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to test an image and output the three relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes two images in the LAB-color-scheme and converts them to RGB before displaying them side-by-side\n",
    "def showNetResults(predictedImage, initialImage): \n",
    "\n",
    "    # get the Black-And-White Version of the Image\n",
    "    l_img = predictedImage[:,:,0]\n",
    "    \n",
    "    # Convert the Initial Image to RGB\n",
    "    initialImage   = color.lab2rgb(initialImage.astype('float64'))\n",
    "    \n",
    "    # Convert the Predicted Image to RGB\n",
    "    predictedImage = color.lab2rgb(predictedImage.astype('float64'))\n",
    "    \n",
    "    # Create a Figure \n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    # Plot the Black-and-White Image \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Black and White Image\")\n",
    "    plt.imshow(l_img, cmap='gray')\n",
    "    \n",
    "    # Plot the Reconstructed / Predicted Image\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Reconstructed Image\")\n",
    "    plt.imshow(predictedImage)\n",
    "    \n",
    "    # Plot the Original / Ground-Truth Image \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Ground Truth Image\")\n",
    "    plt.imshow(initialImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data and split into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the names of the images in that directory and shuffle the names\n",
    "input_images = np.asarray([x for x in os.listdir(input_directory) if x.endswith(\".jpg\")])\n",
    "np.random.shuffle(input_images)\n",
    "num_inputs = len(input_images)\n",
    "\n",
    "# separate data into training, validation, and testing\n",
    "train_X, val_X, test_X = np.split(input_images, [int(.9*len(input_images)), int(.95*len(input_images))])\n",
    "num_train = len(train_X)\n",
    "num_val   = len(val_X)\n",
    "num_test  = len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes indices and outputs those images (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function just gets the images at the given indices and outputs them \n",
    "def get_examples(indices): \n",
    "    output = np.zeros((input_size, input_size, 3, len(indices)), 'uint8')\n",
    "    for i, n in enumerate(indices):\n",
    "        im =  io.imread(input_directory + input_images[n])\n",
    "        output[:,:,:,i] = im\n",
    "    return output\n",
    "\n",
    "#  test this function and show the result\n",
    "ind = np.random.choice(num_inputs, 2)\n",
    "examples = get_examples(ind)\n",
    "im = examples[:,:,:,0]\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes a batch size and type of batch (\"train\", \"val\", \"test\") and outputs the input / output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get batch\n",
    "#     num_samples --> the batch size\n",
    "#     batch_type  --> can take the values (train, val, test)\n",
    "def get_batch(num_samples, batch_type): \n",
    "    assert(num_samples <= 100)\n",
    "    \n",
    "    # Create the input / output arrays to be filled with the proper data type \n",
    "    batch_input  = np.zeros((num_samples, input_size, input_size, 1), 'float32')\n",
    "    batch_output = np.zeros((num_samples, input_size, input_size, 3), 'float32')\n",
    "    \n",
    "    # Select the indices based on whether we are getting a Training/Validation/Testing set\n",
    "    if batch_type == \"test\": \n",
    "        batch = np.random.choice(num_test, num_samples)\n",
    "    elif batch_type == 'val':\n",
    "        batch = np.random.choice(num_val, num_samples)\n",
    "    else: \n",
    "        batch = np.random.choice(num_train, num_samples)\n",
    "    \n",
    "    # Enumerate through the batch and fill the array with the proper data\n",
    "    for i, n in enumerate(batch):\n",
    "        \n",
    "        # get the data from the data of the proper type: Training/Validation/Testing \n",
    "        if batch_type == \"test\": \n",
    "            im =  color.rgb2lab(io.imread(input_directory+test_X[n]))\n",
    "        elif batch_type == 'val':\n",
    "            im =  color.rgb2lab(io.imread(input_directory+val_X[n]))\n",
    "        else: \n",
    "            im =  color.rgb2lab(io.imread(input_directory+train_X[n]))\n",
    "        \n",
    "        # put the L channel in the input and the whole image in the output \n",
    "        batch_input[i,:,:,0] = im[:,:,0]\n",
    "        batch_output[i,:,:,:] = im\n",
    "    \n",
    "    return batch_input, batch_output\n",
    "\n",
    "# call to the function \n",
    "[batch_input, batch_output] = get_batch(2, 'train')\n",
    "print(batch_input.shape)\n",
    "print(batch_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the AutoEncoder network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill-in-the-blanks for the VGG pre-trained network \n",
    "def vgg_net(weights, image):\n",
    "    layers = (\n",
    "        # 'conv1_1', 'relu1_1',\n",
    "        'conv1_2', 'relu1_2', 'pool1',\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "\n",
    "    net = {}\n",
    "    current = image\n",
    "    for i, name in enumerate(layers):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i + 2][0][0][0][0]\n",
    "            kernels = utils.get_variable(np.transpose(kernels, (1, 0, 2, 3)), name=name + \"_w\")\n",
    "            bias = utils.get_variable(bias.reshape(-1), name=name + \"_b\")\n",
    "            current = utils.conv2d_basic(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current, name=name)\n",
    "        elif kind == 'pool':\n",
    "            current = utils.avg_pool_2x2(current)\n",
    "        net[name] = current\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that builds the rest of the net\n",
    "def generator(images, train_phase):\n",
    "    \n",
    "    # Ge the model data and set up\n",
    "    print(\"setting up vgg initialized conv layers ...\")\n",
    "    model_data = utils.get_model_data(model_dir, model_url)\n",
    "    weights = np.squeeze(model_data['layers'])\n",
    "\n",
    "    # Build the remaining \"decoder\" that will colorize the image\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        \n",
    "        # First Layer: 3x3 2dConv with bias follower by RELU\n",
    "        #              Need this layer because the input is only 1 channel\n",
    "        W0 = utils.weight_variable([3, 3, 1, 64], name=\"W0\")\n",
    "        b0 = utils.bias_variable([64], name=\"b0\")\n",
    "        conv0 = utils.conv2d_basic(images, W0, b0)\n",
    "        hrelu0 = tf.nn.relu(conv0, name=\"relu\")\n",
    "\n",
    "        # Add in the VGG network \n",
    "        image_net = vgg_net(weights, hrelu0)\n",
    "        vgg_final_layer = image_net[\"relu5_3\"]\n",
    "        pool5 = utils.max_pool_2x2(vgg_final_layer)\n",
    "        \n",
    "        # Decoder Level 1: begin to upscale the image and decrease the number of filters \n",
    "        #                  Use conv2d_transpose_strided() with 4x4 filter\n",
    "        deconv_shape1 = image_net[\"pool4\"].get_shape()\n",
    "        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, pool5.get_shape()[3].value], name=\"W_t1\")\n",
    "        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=\"b_t1\")\n",
    "        conv_t1 = utils.conv2d_transpose_strided(pool5, W_t1, b_t1, output_shape=tf.shape(image_net[\"pool4\"]))\n",
    "        fuse_1 = tf.add(conv_t1, image_net[\"pool4\"], name=\"fuse_1\")\n",
    "\n",
    "        # Decoder Level 2: continue to upscale the image and decrease the number of filters \n",
    "        deconv_shape2 = image_net[\"pool3\"].get_shape()\n",
    "        print(deconv_shape2)\n",
    "        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=\"W_t2\")\n",
    "        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=\"b_t2\")\n",
    "        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(image_net[\"pool3\"]))\n",
    "        fuse_2 = tf.add(conv_t2, image_net[\"pool3\"], name=\"fuse_2\")\n",
    "        \n",
    "        # Decoder Level 3: continue to upscale the image and decrease the number of filters \n",
    "        shape = tf.shape(images)\n",
    "        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], 2])\n",
    "        W_t3 = utils.weight_variable([16, 16, 2, deconv_shape2[3].value], name=\"W_t3\")\n",
    "        b_t3 = utils.bias_variable([2], name=\"b_t3\")\n",
    "        pred = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=8)\n",
    "\n",
    "    # return the concatenation of the input with the output to make it the full image\n",
    "    return tf.concat(axis=3, values=[images, pred], name=\"pred_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the training that the net will under go\n",
    "def train(loss, var_list):\n",
    "    # create and AdamOptimizer with a learning rate and beta parameter\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta)\n",
    "    \n",
    "    # compute the gradients\n",
    "    grads = optimizer.compute_gradients(loss, var_list=var_list)\n",
    "    \n",
    "    # Apply the gradients to the optimizer\n",
    "    return optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Setting up network...\")\n",
    "\n",
    "# Create placeholders for the input images and the output images \n",
    "train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "images = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='L_image')\n",
    "lab_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\"LAB_image\")\n",
    "\n",
    "# set pred_images to the output of the network \n",
    "pred_image = generator(images, train_phase)\n",
    "\n",
    "# define the loss functions that we are minimizing as the between the images \n",
    "gen_loss_mse   = tf.reduce_mean(2 * tf.nn.l2_loss(pred_image - lab_images))\n",
    "gen_loss_huber = tf.reduce_mean(tf.losses.huber_loss(lab_images, pred_image, delta=5.0))\n",
    "gen_loss_pmse  = tf.reduce_mean(tf.losses.mean_pairwise_squared_error(lab_images, pred_image))\n",
    "\n",
    "# set gen_loss to the loss function that we will actually be using\n",
    "gen_loss = gen_loss_pmse\n",
    "\n",
    "# initialize training variables\n",
    "train_variables = tf.trainable_variables()\n",
    "\n",
    "# train_op (which will be passes into the network) --> call the train() function\n",
    "train_op = train(gen_loss, train_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare these outside so that we can continue to plot the training for multiple iterations\n",
    "avg_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# we need the saver for model saving / restoring \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of batches per epoch\n",
    "batch_per_ep = int(num_train / batch_size)\n",
    "\n",
    "# variable for the loss \n",
    "train_loss = 0.0\n",
    "\n",
    "# begin tensor flow session\n",
    "with tf.Session() as sess:\n",
    "    # initialize variables \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # get the saved model if it exists \n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoints)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Restoring Model\")\n",
    "    else: \n",
    "        print(\"Creating New Model\")\n",
    "\n",
    "    # iterate through the number of epochs\n",
    "    for ep in range(epoch_num): \n",
    "\n",
    "        # iterate through the batches per epoch \n",
    "        for batch_n in range(batch_per_ep): \n",
    "            \n",
    "            # get the batch out \n",
    "            l_image, color_images = get_batch(batch_size, \"train\")\n",
    "            \n",
    "            # get the dictionary to feed into the training\n",
    "            feed_dict = {images: l_image, lab_images: color_images, train_phase: True}\n",
    "            \n",
    "            # run the dictionary through the network and output the mean-squared-error\n",
    "            _, train_loss = sess.run([train_op, gen_loss], feed_dict=feed_dict)\n",
    "            \n",
    "            if batch_n % 10 == 0: \n",
    "                print(\"Epoch: %d, Batch: %d, Pairwise MSE Loss: %g\" % (ep, batch_n, train_loss))\n",
    "        \n",
    "        # save the model each epoch\n",
    "        _ = saver.save(sess, checkpoints + \"model.ckpt\")\n",
    "\n",
    "        # get error for validation set\n",
    "        l_image, color_images = get_batch(batch_size, \"val\")\n",
    "        feed_dict = {images: l_image, lab_images: color_images, train_phase: True}\n",
    "        _, val_loss = sess.run([train_op, gen_loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # plot the training and validation error\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        avg_loss.append(train_loss)\n",
    "        val_loss.append(val_loss)\n",
    "        train_plt = plt.plot(avg_loss, label=\"Train\")\n",
    "        val_plt   = plt.plot(val_loss, label=\"Val\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Pairwise MSE Loss Over Time\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.figure()\n",
    "        plt.show()\n",
    "        if ep % 20 == 19: \n",
    "            lr = lr / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the batch and run it through the network\n",
    "with tf.Session() as sess:\n",
    "    # get the previous model \n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoints)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else: \n",
    "        # this should never fail\n",
    "        assert(False)  \n",
    "    \n",
    "    # run num_tests through the network and then display them \n",
    "    num_tests = 5\n",
    "    l_image, color_images = get_batch(num_tests, \"test\")\n",
    "    feed_dict = {images: l_image, lab_images: color_images, train_phase: False}\n",
    "    [pred, loss] = sess.run([pred_image, gen_loss], feed_dict=feed_dict)\n",
    "    print(loss)\n",
    "    for i in range(num_tests): \n",
    "        showNetResults(pred[i,:,:,:], color_images[i,:,:,:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
